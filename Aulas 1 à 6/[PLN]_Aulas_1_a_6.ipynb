{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1eNVyoWqSRh7Y_rADwVcRtdmK-Si1szFg",
      "authorship_tag": "ABX9TyPGHwOpuTZB1wi4Qye3Oq1S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovannaRochaDSM/fatec_PLN_Codes/blob/master/Aulas%201%20%C3%A0%206/%5BPLN%5D_Aulas_1_a_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Programação de Linguagem Natural - Aulas 01 a 06**"
      ],
      "metadata": {
        "id": "FZETSVODiLsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Nome:** Giovanna da Rocha Machado | **RA:** 1131392213024"
      ],
      "metadata": {
        "id": "3j-otrb8iRX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nas aulas 01 a 06, vimos práticas e exemplos relacionados ao processamento de linguagem natural (PLN), incluindo tokenização, semântica, limpeza de dados, vetorização e representação de palavras. Cada aula apresenta exemplos e códigos práticos para ajudar no aprendizado.\n",
        "\n"
      ],
      "metadata": {
        "id": "zBwAQgjoiJnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aula 01 e 02 - Spacy e NLTK**\n",
        "\n"
      ],
      "metadata": {
        "id": "XmuX7EnhzkYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 1 - Sintaxe\n",
        "\n",
        "#### Demonstração de segmentação, tokenização e uma árvore de dependência\n"
      ],
      "metadata": {
        "id": "lxKKB22uzjfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy # Importação da lib\n",
        "\n",
        "npl = spacy.load(\"en_core_web_sm\") # Modelo pré treinado\n",
        "\n",
        "frase = npl(\"The foxy jumps over the lazy dog. The black cat is walking. The white horse is sleeping.\")\n",
        "\n",
        "print(frase)\n",
        "\n",
        "#print(\"\\n Segmentação das frases...\")\n",
        "#for sentenca in frase.sents:\n",
        " # print(sentenca.text)\n",
        "\n",
        "for token in frase:\n",
        "  print(token.text, end=' ') # Visualização de cada palavra\n",
        "  print(token.pos_, end=' ') # Visualização das classes gramaticais\n",
        "  print(token.dep_) # Visualização das relações das palavras dentro da frase\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "displacy.render(frase, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "1_Adps850Otk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "outputId": "d5618ddd-498e-441a-becf-2ee3efcba29a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The foxy jumps over the lazy dog. The black cat is walking. The white horse is sleeping.\n",
            "The DET det\n",
            "foxy NOUN nsubj\n",
            "jumps VERB ROOT\n",
            "over ADP prep\n",
            "the DET det\n",
            "lazy ADJ amod\n",
            "dog NOUN pobj\n",
            ". PUNCT punct\n",
            "The DET det\n",
            "black ADJ amod\n",
            "cat NOUN nsubj\n",
            "is AUX aux\n",
            "walking VERB ROOT\n",
            ". PUNCT punct\n",
            "The DET det\n",
            "white ADJ amod\n",
            "horse NOUN nsubj\n",
            "is AUX aux\n",
            "sleeping VERB ROOT\n",
            ". PUNCT punct\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"19002da767b24385b982395874c85b1a-0\" class=\"displacy\" width=\"3025\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">foxy</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">jumps</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">over</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">lazy</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">dog.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">The</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">black</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">cat</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">walking.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">The</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">white</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">horse</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">sleeping.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-19002da767b24385b982395874c85b1a-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-19002da767b24385b982395874c85b1a-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-19002da767b24385b982395874c85b1a-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-19002da767b24385b982395874c85b1a-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-19002da767b24385b982395874c85b1a-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-19002da767b24385b982395874c85b1a-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-19002da767b24385b982395874c85b1a-0-3\" stroke-width=\"2px\" d=\"M770,264.5 C770,89.5 1095.0,89.5 1095.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-19002da767b24385b982395874c85b1a-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-19002da767b24385b982395874c85b1a-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-19002da767b24385b982395874c85b1a-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-19002da767b24385b982395874c85b1a-0-5\" stroke-width=\"2px\" d=\"M595,264.5 C595,2.0 1100.0,2.0 1100.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-19002da767b24385b982395874c85b1a-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1100.0,266.5 L1108.0,254.5 1092.0,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-19002da767b24385b982395874c85b1a-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,89.5 1620.0,89.5 1620.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-19002da767b24385b982395874c85b1a-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-19002da767b24385b982395874c85b1a-0-7\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,177.0 1615.0,177.0 1615.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-19002da767b24385b982395874c85b1a-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1470,266.5 L1462,254.5 1478,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-19002da767b24385b982395874c85b1a-0-8\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,89.5 1970.0,89.5 1970.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-19002da767b24385b982395874c85b1a-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-19002da767b24385b982395874c85b1a-0-9\" stroke-width=\"2px\" d=\"M1820,264.5 C1820,177.0 1965.0,177.0 1965.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-19002da767b24385b982395874c85b1a-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1820,266.5 L1812,254.5 1828,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-19002da767b24385b982395874c85b1a-0-10\" stroke-width=\"2px\" d=\"M2170,264.5 C2170,89.5 2495.0,89.5 2495.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-19002da767b24385b982395874c85b1a-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2170,266.5 L2162,254.5 2178,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-19002da767b24385b982395874c85b1a-0-11\" stroke-width=\"2px\" d=\"M2345,264.5 C2345,177.0 2490.0,177.0 2490.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-19002da767b24385b982395874c85b1a-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2345,266.5 L2337,254.5 2353,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-19002da767b24385b982395874c85b1a-0-12\" stroke-width=\"2px\" d=\"M2520,264.5 C2520,89.5 2845.0,89.5 2845.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-19002da767b24385b982395874c85b1a-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2520,266.5 L2512,254.5 2528,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-19002da767b24385b982395874c85b1a-0-13\" stroke-width=\"2px\" d=\"M2695,264.5 C2695,177.0 2840.0,177.0 2840.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-19002da767b24385b982395874c85b1a-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2695,266.5 L2687,254.5 2703,254.5\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 1.a - Em português"
      ],
      "metadata": {
        "id": "JEGvt4000cQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Carregar o modelo para português\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "# Processar um texto em português\n",
        "textoRecebido = input(\"Digite um texto para ser analisado: \")\n",
        "doc = nlp(textoRecebido)\n",
        "\n",
        "print('\\nAnálise gramatical das palavras:')\n",
        "for token in doc:\n",
        "    print(f\"Palavra: {token.text}, Classe: {token.pos_}\")\n",
        "\n",
        "print(\"\\nAnalise de Dependências:\")\n",
        "for token in doc:\n",
        "  print(f\"Palavra: {token.text}, Depende de: {token.head.text}\")\n",
        "\n",
        "# Visualizar a árvore graficamente (opcional)\n",
        "from spacy import displacy\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "MwXj9phq0erj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Spacy** - Visão Geral\n",
        "\n",
        "### Português\n",
        "nlp_pt = spacy.load('pt_core_news_sm')\n",
        "\n",
        "###Inglês\n",
        "nlp_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "###Espanhol\n",
        "nlp_es = spacy.load('es_core_news_sm')\n",
        "\n",
        "### Francês\n",
        "nlp_fr = spacy.load('fr_core_news_sm')\n",
        "\n",
        "###Alemão\n",
        "nlp_de = spacy.load('de_core_news_sm')\n",
        "\n",
        "Os modelos pré-treinados são como \"cérebros artificiais\" que já foram ensinados a entender e analisar um idioma específico. Vou explicar em detalhes:\n",
        "\n",
        "O que são:\n",
        "\n",
        "- Conjuntos de dados estatísticos e regras\n",
        "- Treinados com milhões de textos\n",
        "- Especializados em tarefas específicas de linguagem\n",
        "- Resultado de aprendizado de máquina\n",
        "\n",
        "Como são treinados:\n",
        "- Alimentados com grande volume de textos\n",
        "- Aprendem padrões do idioma\n",
        "- Reconhecem estruturas gramaticais\n",
        "- Identificam relações entre palavras\n",
        "- São testados e refinados\n",
        "\n",
        "Tipos de modelos por tamanho:\n",
        "\n",
        "Pequeno (sm):\n",
        "- Mais rápido\n",
        "- Menor precisão\n",
        "- Usa menos memória\n",
        "- Bom para testes\n",
        "\n",
        "Médio (md):\n",
        "- Equilíbrio entre velocidade e precisão\n",
        "- Precisão moderada\n",
        "- Bom para uso geral\n",
        "\n",
        "Grande (lg):\n",
        "- Mais preciso\n",
        "- Mais lento\n",
        "- Usa mais memória\n",
        "- Melhor para análises detalhadas\n",
        "\n",
        "Vantagens de usar modelos pré-treinados:\n",
        "- Não precisa treinar do zero\n",
        "- Economia de tempo e recursos\n",
        "- Resultados consistentes\n",
        "- Já testados e validados\n",
        "- Atualizados regularmente"
      ],
      "metadata": {
        "id": "q9B7nXEy0sHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 2 -  Tokenização e Segmentação com NLTK"
      ],
      "metadata": {
        "id": "5-npZXkn1eY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download necessário para usar os tokenizadores\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Texto de exemplo\n",
        "texto = \"O gato preto pulou sobre o muro. Ele estava com fome e procurava algo para comer.\"\n",
        "print(texto)\n",
        "\n",
        "# Segmentação de frases\n",
        "frases = sent_tokenize(texto)\n",
        "print(\"\\nSegmentação de Frases:\")\n",
        "print(frases)\n",
        "\n",
        "# Tokenização de palavras para cada frase\n",
        "print(\"\\nTokenização de Palavras:\")\n",
        "for frase in frases:\n",
        "    palavras = word_tokenize(frase)\n",
        "    print(palavras)\n"
      ],
      "metadata": {
        "id": "CaXjC4Bq1hYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 1"
      ],
      "metadata": {
        "id": "-nQmWCxKkJOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importar o modelo de vetorização da biblioteca gensin\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Crie uma lista de FRASES (listas de palavras)\n",
        "sentences = [\n",
        "    [\"O\", \"gato\", \"subiu\", \"na\", \"árvore\"],# sentences [0][2] subiu\n",
        "    [\"O\", \"cachorro\", \"latiu\", \"para\", \"o\", \"gato\"],\n",
        "    [\"A\", \"bola\", \"rolou\", \"pelo\", \"gramado\"]\n",
        "]\n",
        "\n",
        "# Treine o modelo Word2Vec\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "\n",
        "# Obtenha o vetor de uma palavra\n",
        "vector = model.wv['gato']\n",
        "# print(vector)\n",
        "\n",
        "# # Calcule a similaridade entre duas palavras\n",
        "similarity = model.wv.similarity('gato', 'gramado')\n",
        "print(similarity)"
      ],
      "metadata": {
        "id": "GcKmAH3P1yZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 2"
      ],
      "metadata": {
        "id": "mPQmIJoL17c7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Mais frases para melhor treinamento\n",
        "sentences = [\n",
        "    [\"O\", \"gato\", \"subiu\", \"na\", \"árvore\"],\n",
        "    [\"O\", \"cachorro\", \"latiu\", \"para\", \"o\", \"gato\"],\n",
        "    [\"A\", \"bola\", \"rolou\", \"pelo\", \"gramado\"],\n",
        "    [\"O\", \"gato\", \"bebeu\", \"leite\"],\n",
        "    [\"O\", \"cachorro\", \"correu\", \"no\", \"parque\"]\n",
        "]\n",
        "\n",
        "# Treinando o modelo com mais parâmetros\n",
        "model = Word2Vec(\n",
        "    sentences,\n",
        "    min_count=1,        # Frequência mínima das palavras\n",
        "    vector_size=100,    # Tamanho do vetor\n",
        "    window=5,           # Tamanho da janela de contexto\n",
        "    sg=0               # 0 para CBOW, 1 para Skip-gram\n",
        ")\n",
        "\n",
        "# Obtendo vetor\n",
        "vector = model.wv['gato']\n",
        "print(\"Vetor da palavra 'gato':\", vector[:5])  # Mostra só os 5 primeiros números\n",
        "\n",
        "# Similaridade\n",
        "similarity = model.wv.similarity('gato', 'cachorro')\n",
        "print(\"Similaridade entre 'gato' e 'cachorro':\", similarity)\n",
        "\n",
        "# Palavras mais similares\n",
        "similar_words = model.wv.most_similar('gato')\n",
        "print(\"Palavras mais similares a 'gato':\", similar_words)"
      ],
      "metadata": {
        "id": "1RIYXFCc18vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numero1 = int(input(\"informe um valor inteiro: \"))\n",
        "numero2 = int(input(\"informe outro valor: \"))\n",
        "soma = numero1 + numero2\n",
        "print(soma)"
      ],
      "metadata": {
        "id": "pBdH8wS11-ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aula 3 - Bibliotecas e Corpora**"
      ],
      "metadata": {
        "id": "1BhdIWoK2b77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 1 - Utilização da biblioteca NLTK"
      ],
      "metadata": {
        "id": "01esZunMi01C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk # importando a biblioteca\n",
        "from nltk.corpus import machado # importanto o corpus machado\n",
        "\n",
        "nltk.download('machado') # fazendo download co corpus\n",
        "\n",
        "# Pegar o nomes dos arquivos que compõem o corpus\n",
        "#machado.fileids()[:10]\n",
        "  # fileides() >>> retorna os ids de todos os documentos de corpus\n",
        "  # [:5] >>> quantidade de resultado - opcional\n",
        "\n",
        "print(machado.raw('contos/macn001.txt'))"
      ],
      "metadata": {
        "id": "R2Wp87Oe2hgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 2 - Contagem de Palavras"
      ],
      "metadata": {
        "id": "bAZ1Qfry2jP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importação da biblioteca que irá disponibilizar os recursos para PLN\n",
        "import nltk\n",
        "\n",
        "# importar a função brown do módulo corpus da biblioteca nltk\n",
        "#   Isso irá permitir acessar o corpus Brown\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# importar a função que permite realizer calculo de frequencia das palavras em um texto\n",
        "from nltk import FreqDist\n",
        "\n",
        "nltk.download('brown') # baixa o corpora indicado caso não tenha no ambiente de desenvolvimento\n",
        "\n",
        "palavras = brown.words(categories='news')\n",
        "\n",
        "print(palavras)\n",
        "\"\"\"\n",
        "brown.word >>> função que retorna uma lista de palavras que estão dentro do corpus\n",
        "categories='news' >>> especificação da categoria dentro do corpus.\n",
        "                      Pode ser omitido, retornando todas as palavras so corpus\n",
        "\"\"\"\n",
        "\n",
        "# FreqDist(palavras) >>> faz o mapeamento das palavras que estão na variavel indicada e gera uma lista com esses valores\n",
        "freq_dist = FreqDist(palavras)\n",
        "\n",
        "# realizar a impressão da lista\n",
        "print(\"As 10 palavras mais frequentes no corpus de noticias:\")\n",
        "print(freq_dist.most_common(10))"
      ],
      "metadata": {
        "id": "_Gu5vZ1-2luN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este script demonstra como utilizar a biblioteca nltk para:\n",
        "\n",
        "* Baixar um corpus de texto (Brown).\n",
        "* Carregar palavras de uma categoria específica (notícias).\n",
        "* Calcular a frequência de cada palavra no corpus.\n",
        "* Identificar e exibir as palavras mais frequentes."
      ],
      "metadata": {
        "id": "SGyqhZoa2nf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 2 - Eliminação de palavras irrelevantes\n",
        "São palavras que não agregam no conteúdo, ou seja, são irrelevantes para a compreensão do conteúdo.\n",
        "Os mecanismos de buscas filtram essas palavras para minimizar a quantidade de trabalho computacional"
      ],
      "metadata": {
        "id": "3M9IeMkr2pXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk # importando a bliblioteca\n",
        "from nltk.corpus import stopwords # importando módulo de funções do corpus stopwords\n",
        "nltk.download('stopwords') # donwload do corpus stopwords\n",
        "\n",
        "stopwords.words('english')\n",
        "  # .words() >>> métodos para acessar a lista de palavras dentro do corpus\n",
        "  # 'portuguese' >>> seleção do idioma"
      ],
      "metadata": {
        "id": "2Fn9-Gc12rfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importação da biblioteca e das funções que serão utilizadas no programa\n",
        "import nltk\n",
        "from nltk import FreqDist # calculo de frequencia\n",
        "from nltk.corpus import brown # função que permite acessar corpora Brown\n",
        "from nltk.corpus import stopwords # função que permite acessar corpora das stopword\n",
        "\n",
        "# Verifica se os corpus estão no ambiente, se não tiver baixa\n",
        "nltk.download('brown')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# carrega as palavras do corpus da categoria noticias\n",
        "palavras = brown.words(categories='news')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "  # stopwords.words('english') busca a lista das palavras do idioma inglês\n",
        "  # set(...) convertendo a lista para conjunto para eliminar redundância\n",
        "    # conjunto não permite duplicidade - trz eficiência na busca\n",
        "\n",
        "# remoção das stopwords\n",
        "palavras_filtradas = [palavra for palavra in palavras if palavra not in stop_words]\n",
        "  # criação de lista usando lis Comprehension\n",
        "    # nova_lista = [expressão for item in iterável if condição]\n",
        "      # expressão >> o que será adicionado na lista\n",
        "      # item >> elementos atual de iteração\n",
        "      # iterável >> sequência a ser iterada (lista, tupla, conjunto, string)\n",
        "      # if item condição >> uma condição que filtra os elementos\n",
        "    # palavras_filtradas = [] >>> criando uma lista nova\n",
        "    # for palavra in palavras >>> rotina de repetição que ira avaliar cada palavra\n",
        "    # if palavras not in stopwords >>> verifica se palavra faz parte do conjunto stop_words    #\n",
        "\n",
        "# mapeamento das palavras\n",
        "freq_dist = FreqDist(palavras_filtradas)\n",
        "\n",
        "# imprimir as palavras\n",
        "print(\"\\n\\nAs 10 palavras mais frequentes no corpus de noticia (sem stopwords):\")\n",
        "print(freq_dist.most_common(10))"
      ],
      "metadata": {
        "id": "GQKUKLn02s2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 3 NLTK - Tokenização\n",
        "#### Forma 1 - word_tokenize"
      ],
      "metadata": {
        "id": "Aksl39Ef2uTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize # importando o tokenizador de palavras\n",
        "nltk.download('punkt')\n",
        "\n",
        "# criando um texto de exemplo\n",
        "# texto = \"White man came across the sea, he brought us pain and misery, he killed our tribes, he killed our creed and he took our game for his own need.\"\n",
        "texto = \"I'm very happy #betterlife @barney :P :D\"\n",
        "palavras = nltk.word_tokenize(texto)\n",
        "print(palavras)"
      ],
      "metadata": {
        "id": "a2mnTVc02vbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Forma 2 - Twitter tokenize"
      ],
      "metadata": {
        "id": "AxkVAZ9B2wq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer # importando o tokenizador de palavras\n",
        "nltk.download('punkt')\n",
        "\n",
        "# criando um texto de exemplo\n",
        "# texto = \"White man came across the sea, he brought us pain and misery, he killed our tribes, he killed our creed and he took our game for his own need.\"\n",
        "\n",
        "texto = \"I'm very happy #betterlife @barney :P :D\"\n",
        "twitterTokenizer = TweetTokenizer()\n",
        "twitterTokenizer.tokenize(texto)\n"
      ],
      "metadata": {
        "id": "lXcS8kIt2x6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aula 4 - Limpeza de dados textuais**"
      ],
      "metadata": {
        "id": "RmJ3Bzyv3Ekz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Normalização de texto e Remoção de ruído\n",
        "\n",
        " Remover caracteres, pontuações, e normalizar o uso de maiúsculas e minúsculas\n",
        "\n"
      ],
      "metadata": {
        "id": "6P9gjBMJ3F0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importar a biblioteca que tras as funcionalidades para expressões regulares\n",
        "import re\n",
        "\n",
        "original = \"Olá!!! Este é um exemplo de texto, com várias PONTUAÇÕES, símbolos #especiais, e LETRAS maiúsculas.\"\n",
        "\n",
        "texto_limpo = re.sub(r'[^A-Za-zÀ-ÿ\\s]', '', original)\n",
        "  # realiza a substituicação dos caracteres especiais\n",
        "  # re.sub(parametro1, parametro2, parametro3) >>> realizar a busca e a substituição\n",
        "    # parametro1: r'[^A-Za-zÀ-ÿ\\s]': é o trecho que irá ser buscado para substituir\n",
        "      # ^A-Za-zÀ-ÿ\\s: os intervalos de texto que serão procurados\n",
        "        # A-Z: busca o intervalo de letras de A até Z\n",
        "        # a-z: busca o intervalo de letras de a até z\n",
        "        # À-ÿ: busca qualquer letra acentuada\n",
        "        # ^: representa a negação dos valores\n",
        "        # [^A-Za-zÀ-ÿ\\s]: busca todos os valores que não são letras (com ou sem acento)\n",
        "        # \\s: deixar os espaços\n",
        "    # parametro2: '' >>> o termo que eu vou substituir, no caso uma string vazia\n",
        "    # parametro3: variável que contém o meu texto\n",
        "    # O QUE É O TERMO R NA EXPRESSÃO REGULAR REGEX - OLHAR NA DOCUMENTAÇÃO\n",
        "\n",
        "texto_normalizado = texto_limpo.lower()\n",
        "\n",
        "print(f'Texto original: {original}')\n",
        "print(f'\\nTexto limpo: {texto_limpo}')\n",
        "print(f'\\nTexto normalizado: {texto_normalizado}')"
      ],
      "metadata": {
        "id": "B_MSyGRs3Gxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Tokenização\n",
        "   Dividir o texto em unidades menores (tokens), palavras ou pontuações\n",
        "\n"
      ],
      "metadata": {
        "id": "J-TnJeYx3IV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokens = word_tokenize(texto_normalizado)\n",
        "\n",
        "print(tokens)\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "id": "xMT8TnEz3JhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Remoção de StopWords\n",
        "\n",
        "Como \"de\", \"a\", \"o\", removidas simplificam texto\n",
        "\n"
      ],
      "metadata": {
        "id": "UqpDWWTM3KyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords_pt = set (stopwords.words('portuguese'))\n",
        "tokens_sem_stopwords = [palavra for palavra in tokens if palavra.lower() not in stopwords_pt]\n",
        "print(tokens_sem_stopwords)\n",
        "print(len(tokens_sem_stopwords))"
      ],
      "metadata": {
        "id": "alrPp8XQ3MAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Stemming e Lematização\n",
        "\n",
        "  Stemming: reduz palavras à seus radicais\n",
        "  \n",
        "Lematização: normaliza palavras para suas formas base, levando em conta contexto e gramática\n",
        "\n"
      ],
      "metadata": {
        "id": "a_hTQ_ba3NU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "nltk.download('rslp')\n",
        "\n",
        "stemmer = RSLPStemmer()\n",
        "stemming = [stemmer.stem(palavra) for palavra in tokens_sem_stopwords]\n",
        "print(stemming)"
      ],
      "metadata": {
        "id": "nImaHtQl3OlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Utilizando todo o processo de hoje"
      ],
      "metadata": {
        "id": "FyqmHLGy3Q3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importação das bibliotecas - Ordem alfabética\n",
        "import nltk\n",
        "import re\n",
        "# Funcionalidades\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Recursos  NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Texto de exemplo\n",
        "texto = \"Este é um exemplo de texto com muitas palavras, algumas repetidas outra @não, e pontuação e outros simbolos que não FAzem parte do texto que desejo . Vamos limpar e organizar esse texto. Há também números 123 e caracteres especiais @#$%.\"\n",
        "\n",
        "# Limpa ruídos e normalização\n",
        "texto_limpo = re.sub(r'[^a-zA-Z]', ' ', texto)\n",
        "texto_lower = texto_limpo.lower()\n",
        "\n",
        "# Tokenização\n",
        "tokens = nltk.word_tokenize(texto_lower)\n",
        "\n",
        "# Remove Stopwords\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "palavras_filtradas = [palavra for palavra in tokens if palavra not in stop_words]\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "palavras_stemizadas = [stemmer.stem(palavra) for palavra in palavras_filtradas]\n",
        "\n",
        "# Resultado final\n",
        "print(palavras_stemizadas)"
      ],
      "metadata": {
        "id": "xNXGn5q73TrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # **Aula 05 - BoW**"
      ],
      "metadata": {
        "id": "aSCqQN1s3Vz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 1: Vetorização de Texto usando CountVectorizer (Bag of Words - BoW)"
      ],
      "metadata": {
        "id": "jhWwAZDg496L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "documentos = [\n",
        "    \"gato e cachorro\",\n",
        "    \"gato brinca com cachorro\",\n",
        "    \"gato e rato\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "x = vectorizer.fit_transform(documentos)\n",
        "\n",
        "print(\"Matriz BoW:\")\n",
        "print(x.toarray())\n",
        "\n",
        "print(f\"Vocabulário: {vectorizer.vocabulary_}\")"
      ],
      "metadata": {
        "id": "a_4RcUrW3XOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 2: Vetorização com BoW e TF-IDF"
      ],
      "metadata": {
        "id": "kjMDoVw25AYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "  # Term Frequency\n",
        "\n",
        "documentos = [\n",
        "    \"O cachorro gosta de passear no parque\",\n",
        "    \"O gato dorme no sofá o dia todo\",\n",
        "    \"Cachorros e gato podem ser bons amigos\"\n",
        "]\n",
        "\n",
        "# Vetorização por bow\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_bow = vectorizer_bow.fit_transform(documentos)\n",
        "\n",
        "# Impressão do vetor e matriz\n",
        "print(f\"Vocabulário Bow: {vectorizer_bow.vocabulary_}\")\n",
        "print(\"Matriz BoW:\")\n",
        "print(X_bow.toarray())\n",
        "\n",
        "# Vetorização IF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer() # Instansiação da Classe\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(documentos) # Vetorização\n",
        "\n",
        "# fit\n",
        "# transform\n",
        "\n",
        "# Impressão dos vetores com frequênca ponderada\n",
        "print(f\"\\nVocabulário TF-IDF: {vectorizer_tfidf.vocabulary_}\")\n",
        "print(\"Matriz TD-IDF\")\n",
        "print(X_tfidf.toarray())"
      ],
      "metadata": {
        "id": "UmwOYSyC3YTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 3: Pré-processamento de Texto (Tokenização, Remoção de Stopwords, Lematização)"
      ],
      "metadata": {
        "id": "9r8wwtpI5CYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "documentos = [\n",
        "  \"Os cachorros são animais muito amigáveis e leais!\",\n",
        "  \"Eu gosto de gatos porque eles são independentes e fofos.\",\n",
        "  \"Cachorros e gatos podem ser ótimos animais de estimaçãos. \"\n",
        "]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set (stopwords. words (\"portuguese\"))\n",
        "\n",
        "def preprocessar_texto(texto):\n",
        "  texto = re. sub(r'[^a-zA-Zá-y\\s]' , '', texto)\n",
        "  tokens_tudo = word_tokenize (texto.lower())\n",
        "  tokens = [palavra for palavra in tokens_tudo if palavra not in stop_words]\n",
        "  tokens_lema = [lemmatizer. lemmatize(palavra) for palavra in tokens]\n",
        "  return ' '. join(tokens_lema)\n",
        "\n",
        "documentos_processados = [preprocessar_texto(doc) for doc in documentos]\n",
        "\n",
        "print ('documentos Pré-processados:')\n",
        "for i, doc in enumerate (documentos_processados) :\n",
        "  print (f\"Documento {i + 1}: {doc}\")\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer. fit_transform(documentos_processados)\n",
        "print(\"\\nVocabulario BoW:\", vectorizer. vocabulary_)\n",
        "print (\"Matriz BoW:\")\n",
        "print (X_bow.toarray())"
      ],
      "metadata": {
        "id": "5eKPqHuC3aB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aula 06 - Word2Vec**"
      ],
      "metadata": {
        "id": "G3hmBRcx3h58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplo 1 - Word2Vec"
      ],
      "metadata": {
        "id": "TVx2YZo_lLL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# texto à analisar\n",
        "corpus = [\n",
        "    [\"o\", \"cachorro\", \"está\", \"latindo\"],\n",
        "    [\"o\", \"gato\", \"está\", \"miando\"]\n",
        "]\n",
        "\n",
        "# treinamento de palavras\n",
        "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, sg=1)\n",
        "  # sentences - indica texto à analisar\n",
        "  # window - faixa de palavras (antes e depois)\n",
        "\n",
        "vector = model.wv['cachorro']\n",
        "print(vector)\n",
        "\n",
        "similarity = model.wv.similarity('cachorro','gato')\n",
        "print(\"\\nSimilaridade entre 'cachoro' e 'gato': \", similarity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eDiosZ-3jA8",
        "outputId": "6fd54ff9-ce4a-49db-b737-44e128d00e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
            " -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
            " -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
            " -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
            " -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
            " -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
            " -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
            "  1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
            " -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
            "  5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
            "  9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
            " -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
            " -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
            "  7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
            "  9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
            "  8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
            "  3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
            " -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
            "  7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
            " -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
            "  7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
            " -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
            " -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
            " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
            " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]\n",
            "\n",
            "Similaridade entre 'cachoro' e 'gato':  0.13149002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exemplo 2 - Wod2Vec"
      ],
      "metadata": {
        "id": "cQr8jOgHlH5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "corpus = [\n",
        "    [\"o\", \"cachorro\", \"está\", \"latindo\", \"no\", \"quintal\"],\n",
        "    [\"o\", \"gato\", \"está\", \"miando\", \"no\", \"telhado\"],\n",
        "    [\"o\", \"pássaro\", \"está\", \"voando\", \"no\", \"céu\"],\n",
        "    [\"a\", \"bola\", \"está\", \"rolando\", \"no\", \"chão\"],\n",
        "    [\"a\", \"criança\", \"está\", \"brincando\", \"com\", \"o\", \"cachorro\"],\n",
        "    [\"o\", \"gato\", \"e\", \"o\", \"rato\", \"são\", \"inimigos\"],\n",
        "    [\"a\", \"água\", \"está\", \"quente\", \"na\", \"caneca\"],\n",
        "    [\"o\", \"sol\", \"está\", \"brilhando\", \"no\", \"céu\"],\n",
        "    [\"a\", \"lua\", \"está\", \"cheia\", \"hoje\"],\n",
        "    [\"a\", \"computador\", \"está\", \"ligado\", \"na\", \"mesa\"]\n",
        "]\n",
        "\n",
        "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "# calcular similaridade\n",
        "print(f\"Similaridade cachorro e gato: {model.wv.similarity('cachorro', 'gato')}\")\n",
        "print(f\"Similaridade cachorro e bola: {model.wv.similarity('cachorro', 'bola')}\")\n",
        "print(f\"Similaridade céu e lua: {model.wv.similarity('céu', 'lua')}\")\n",
        "print(f\"Similaridade computador e mesa: {model.wv.similarity('computador', 'mesa')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce5_bzFvCgzx",
        "outputId": "e3f37e55-04d7-4f17-d9eb-7053e93931ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similaridade cachorro e gato: -0.027537165209650993\n",
            "Similaridade cachorro e bola: 0.08071544766426086\n",
            "Similaridade céu e lua: 0.16293543577194214\n",
            "Similaridade computador e mesa: 0.037479717284440994\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exemplo -  GloVe (Global Vectors for Word Representation)"
      ],
      "metadata": {
        "id": "xqz03_rsa1vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa libs\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Indica caminho do arquivo\n",
        "glove_path = '/content/drive/MyDrive/Colab Notebooks/glove.6B.100d.txt'\n",
        "\n",
        "# Acesso ao modelo treinado\n",
        "glove_model = KeyedVectors.load_word2vec_format(glove_path, binary=False, no_header=True)\n",
        "  # glove_path - caminho do arquivo\n",
        "  # binary - arquivo em texto\n",
        "  # no_header - ignora cabeçalho\n",
        "\n",
        "# método de proximidade para determnada palavra\n",
        "similaridade = glove_model.similarity('king', 'queen')\n",
        "print(\"Similaridade entre 'king' e 'queen': \", similaridade)\n",
        "\n",
        "palavras_proximas = glove_model.most_similar('king')\n",
        "print(\"Palavras próximas de 'king': \", palavras_proximas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keE0PR6pUD-Z",
        "outputId": "9a063858-4547-4838-a5d5-fba76943f946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similaridade entre 'king' e 'queen':  0.7507691\n",
            "Palavras próximas de 'king':  [('prince', 0.7682328820228577), ('queen', 0.7507690787315369), ('son', 0.7020888328552246), ('brother', 0.6985775232315063), ('monarch', 0.6977890729904175), ('throne', 0.6919989585876465), ('kingdom', 0.6811409592628479), ('father', 0.6802029013633728), ('emperor', 0.6712858080863953), ('ii', 0.6676074266433716)]\n"
          ]
        }
      ]
    }
  ]
}